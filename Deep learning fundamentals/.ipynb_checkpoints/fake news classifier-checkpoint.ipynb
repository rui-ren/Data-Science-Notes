{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "# Bidirectional Encoder Representations from Transformers\n",
    "# BERT = Encoder of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model\n",
    "### Next Sentence Prediction, NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Chatbox with Keras.ipynb', 'cnn.md', 'fake news classifier.ipynb', 'image_processing.py', 'industry_machine_learning.md', 'machine_learning_case_with_tensorflow.md', 'map.py', 'MNIST_data', 'movie_dialog.csv', 'NLP.md', 'object_detection.md', 'ResNet.md', 'Seq2Seq.md', 'sms-spam.csv', 'SqueezeNet.md', 'tensorflow.md', 'text classification.md', 'tokenizer_2grams.py', 'Untitled.ipynb', 'YOLOv1.md']\n"
     ]
    }
   ],
   "source": [
    "cd = os.getcwd()\n",
    "print(os.listdir(cd))\n",
    "os.chdir('C:/Users/ruire/Documents/GitHub/NLP/fake_news_classification/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd443aeb362c4dfeb580eaa618484ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of the whole tokenizer: 30522\n",
      "token               index          \n",
      "-------------------------\n",
      "[unused299]           304\n",
      "cannot               3685\n",
      "lbs                 20702\n",
      "amateur              5515\n",
      "##ular               7934\n",
      "bert                14324\n",
      "winthrop            28974\n",
      "query               23032\n",
      "mysteriously        29239\n",
      "##leigh             13615\n"
     ]
    }
   ],
   "source": [
    "# Use the BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab = tokenizer.vocab\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "# PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "print ('The number of the whole tokenizer:', len(tokenizer.vocab))\n",
    "\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT special token\n",
    "\n",
    "- [CLS] End of sequence\n",
    "- [SEP] Token to one word\n",
    "- [UNK] Unknow token\n",
    "- [PAD] zero padding\n",
    "- [MASK] training mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " '[',\n",
       " 'sep',\n",
       " ']',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson? [Sep] Jim Henson was a puppeteer[SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# text = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['su']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('su')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = '../data/train.csv'\n",
    "train = pd.read_csv('train.csv', index_col='id')\n",
    "test = pd.read_csv('test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "9      6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "9   \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "2   Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "9   It took 30 years of cooking oil to know that o...     agreed  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>321187</td>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321190</td>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321189</td>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321193</td>\n",
       "      <td>167564</td>\n",
       "      <td>160994</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The hanging Saddam is a surrogate? This man's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321191</td>\n",
       "      <td>167564</td>\n",
       "      <td>15084</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>Chinese loquat loquat plaster in America? Pure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tid1    tid2                        title1_zh  \\\n",
       "id                                                        \n",
       "321187  167562   59521  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321190  167564   91315              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321189  167563  167564    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "321193  167564  160994              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321191  167564   15084              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                          title2_zh  \\\n",
       "id                                    \n",
       "321187  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321193  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！   \n",
       "321191         中国川贝枇杷膏在美国受到热捧？纯属谣言！   \n",
       "\n",
       "                                                title1_en  \\\n",
       "id                                                          \n",
       "321187  egypt 's presidential election failed to win m...   \n",
       "321190  A message from Saddam Hussein after he was cap...   \n",
       "321189  Will the United States wage war on Iraq withou...   \n",
       "321193  A message from Saddam Hussein after he was cap...   \n",
       "321191  A message from Saddam Hussein after he was cap...   \n",
       "\n",
       "                                                title2_en  \n",
       "id                                                         \n",
       "321187  Lyon! Lyon officials have denied that Felipe F...  \n",
       "321190  The Top 10 Americans believe that the Lizard M...  \n",
       "321189  A message from Saddam Hussein after he was cap...  \n",
       "321193  The hanging Saddam is a surrogate? This man's ...  \n",
       "321191  Chinese loquat loquat plaster in America? Pure...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320552"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Choose the Chinese version of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title1_zh',\n",
    "        'title2_zh',\n",
    "        'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add new label into the test file\n",
    "test = test.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                   title2_zh      label\n",
       "id                                                                          \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……  unrelated\n",
       "2   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  unrelated\n",
       "9                \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     agreed"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>321187</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321190</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321189</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321193</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321191</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title1_zh                    title2_zh  label\n",
       "id                                                                         \n",
       "321187  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？    NaN\n",
       "321190              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国    NaN\n",
       "321189    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思    NaN\n",
       "321193              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！    NaN\n",
       "321191              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！    NaN"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the test size:  80126\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the test size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of training size and test size:  4.000599056486035\n"
     ]
    }
   ],
   "source": [
    "ratio = len(train) / len(test)\n",
    "print(\"The ratio of training size and test size: \", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title1_zh    False\n",
       "title2_zh     True\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna('UNKNOWN', inplace=True)\n",
    "test.fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 3)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '水',\n",
       " '[MASK]',\n",
       " '[UNK]',\n",
       " '，',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '道',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '子',\n",
       " '。']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\n",
      "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '水', '[MASK]', '[UNK]', '，', '[UNK]', '[UNK]'] ...\n",
      "[101, 100, 100, 100, 1893, 103, 100, 1989, 100, 100] ...\n",
      "PyTorch version 1.0.0.dev20190328\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(tokens[:10], '...')\n",
    "print(ids[:10], '...')\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. remove the too long sample, otherwise GPU will burn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_title = (train['title2_zh'].isnull())|(train['title1_zh'].isnull())|(train['title2_zh'] == '')|(train['title2_zh'] == '0')\n",
    "\n",
    "train = train[~empty_title]\n",
    "MAX_LENGTH = 30\n",
    "train = train[~(train.title1_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "train = train[~(train.title2_zh.apply(lambda x: len(x)) > MAX_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239119, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239119"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrelated    0.678691\n",
       "agreed       0.294753\n",
       "disagreed    0.026556\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train.label)\n",
    "train.label.value_counts()/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-chinese-vocab.txt', 'model', 'output', 'run_classifier.py', 'sample_submission.csv', 'test.csv', 'train.csv', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "cd = os.getcwd()\n",
    "print(os.listdir(cd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "train, val = train_test_split(\n",
    "    train,\n",
    "    test_size = VALIDATION_RATIO,\n",
    "    random_state=RANDOM_STATE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['unrelated', 'agreed', 'disagreed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training set and test set ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_classifier import *\n",
    "train_examples = [InputExample('train', row.title1_zh, row.title2_zh, row.label)\n",
    "                 for row in train.itertuples()]\n",
    "\n",
    "val_examples = [InputExample('val', row.title1_zh, row.title2_zh, row.label)\n",
    "               for row in val.itertuples()]\n",
    "\n",
    "test_examples = [InputExample('test', row.title1_zh, row.title2_zh, 'unrelated')\n",
    "                for row in test.itertuples()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215207"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_total = len(train_examples)\n",
    "train_examples = train_examples[:int(original_total*0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here Turn on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 32\n",
    "train_batch_size = train_batch_size // gradient_accumulation_steps\n",
    "output_dir = 'output'\n",
    "bert_model = 'bert-base-chinese'\n",
    "num_train_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int(\n",
    "    len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'model'\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "max_seq_length = 128\n",
    "label_list = ['unrelated', 'agreed', 'disagreed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 16:18:26 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file C:/Users/ruire/Documents/GitHub/NLP/fake_news_classification/data/bert-base-chinese-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "VOCAB = \"C:/Users/ruire/Documents/GitHub/NLP/fake_news_classification/data/bert-base-chinese-vocab.txt\"\n",
    "MODEL = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 16:18:26 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at model\\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "02/23/2020 16:18:26 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file model\\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir C:\\Users\\ruire\\AppData\\Local\\Temp\\tmp9ql3i93f\n",
      "02/23/2020 16:18:29 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "02/23/2020 16:18:33 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/23/2020 16:18:33 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL,\n",
    "                                                      cache_dir = 'model',\n",
    "                                                      num_labels = 3)\n",
    "model.to(device)\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x17acb7cd978>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=learning_rate,\n",
    "                    warmup=warmup_proportion,\n",
    "                    t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_step = 0\n",
    "# nb_tr_steps = 0\n",
    "# tr_loss = 0\n",
    "\n",
    "# Using Hugging face, convert_examples_to_features \n",
    "# add new information\n",
    "\n",
    "# train_features = convert_examples_to_features(\n",
    "#     train_examples, label_list, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 16:18:43 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   guid: train\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   tokens: [CLS] 厉 害 了 国 内 75 岁 大 爷 发 明 隐 形 衣 ， 穿 上 瞬 间 消 失 ？ [SEP] 马 云 重 磅 揭 秘 ： 三 马 新 玩 法 的 暴 富 行 业 爆 料 ， 三 军 对 垒 即 将 来 袭 ！ 6 [SEP]\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_ids: 101 1326 2154 749 1744 1079 8273 2259 1920 4267 1355 3209 7391 2501 6132 8024 4959 677 4746 7313 3867 1927 8043 102 7716 756 7028 4829 2999 4908 8038 676 7716 3173 4381 3791 4638 3274 2168 6121 689 4255 3160 8024 676 1092 2190 1799 1315 2199 3341 6159 8013 127 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   guid: train\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   tokens: [CLS] 网 传 飞 机 撒 药 治 白 蛾 乌 鲁 木 齐 园 林 部 门 回 应 ： 假 消 息 [SEP] 辟 谣 ｜ 网 传 [UNK] 福 州 园 林 办 通 知 ： 飞 机 撒 药 治 白 蛾 [UNK] 系 谣 言 [SEP]\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_ids: 101 5381 837 7607 3322 3054 5790 3780 4635 6042 723 7826 3312 7970 1736 3360 6956 7305 1726 2418 8038 969 3867 2622 102 6792 6469 8078 5381 837 100 4886 2336 1736 3360 1215 6858 4761 8038 7607 3322 3054 5790 3780 4635 6042 100 5143 6469 6241 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   guid: train\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   tokens: [CLS] 农 村 大 妈 的 [UNK] 土 方 法 [UNK] ， 治 好 了 大 多 数 人 的 腰 间 盘 突 出 ！ [SEP] 农 村 的 这 个 东 西 是 治 疗 腰 间 盘 突 出 的 克 星 ， 当 天 见 效 ， 一 治 一 个 好 ！ [SEP]\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_ids: 101 1093 3333 1920 1968 4638 100 1759 3175 3791 100 8024 3780 1962 749 1920 1914 3144 782 4638 5587 7313 4669 4960 1139 8013 102 1093 3333 4638 6821 702 691 6205 3221 3780 4545 5587 7313 4669 4960 1139 4638 1046 3215 8024 2496 1921 6224 3126 8024 671 3780 671 702 1962 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   guid: train\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   tokens: [CLS] 舒 淇 被 曝 与 侯 孝 贤 开 房 最 新 进 展 更 新 微 博 倡 节 约 [SEP] 不 能 忍 ！ 某 媒 体 谈 白 百 何 时 指 舒 淇 与 侯 孝 贤 开 房 ， 发 声 明 严 厉 谴 责 ！ [SEP]\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_ids: 101 5653 3899 6158 3284 680 908 2105 6570 2458 2791 3297 3173 6822 2245 3291 3173 2544 1300 956 5688 5276 102 679 5543 2556 8013 3378 2054 860 6448 4635 4636 862 3198 2900 5653 3899 680 908 2105 6570 2458 2791 8024 1355 1898 3209 698 1326 6482 6569 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   guid: train\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   tokens: [CLS] 一 个 咸 丰 通 宝 拾 文 可 以 换 一 栋 别 墅 [SEP] 一 枚 咸 丰 重 宝 的 价 值 换 了 一 栋 漂 亮 的 别 墅 [SEP]\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_ids: 101 671 702 1496 705 6858 2140 2896 3152 1377 809 2940 671 3406 1166 1863 102 671 3361 1496 705 7028 2140 4638 817 966 2940 749 671 3406 4023 778 4638 1166 1863 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 16:18:43 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "02/23/2020 16:18:55 - INFO - run_classifier -   **** Running training ****\n",
      "02/23/2020 16:18:55 - INFO - run_classifier -    Num example = 43041\n",
      "02/23/2020 16:18:55 - INFO - run_classifier -    Batch size = 8\n",
      "02/23/2020 16:18:55 - INFO - run_classifier -    Num steps =16140\n"
     ]
    }
   ],
   "source": [
    "# record the number of training and test information\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "\n",
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, max_seq_length, tokenizer)\n",
    "logger.info(\"**** Running training ****\")\n",
    "logger.info(\" Num example = %d\", len(train_examples))\n",
    "logger.info(\" Batch size = %d\", train_batch_size)\n",
    "logger.info(\" Num steps =%d\", num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/5380)\n",
      "Fininshed: 10.00% (538/5380)\n",
      "Fininshed: 20.00% (1076/5380)\n",
      "Fininshed: 30.00% (1614/5380)\n",
      "Fininshed: 40.00% (2152/5380)\n",
      "Fininshed: 50.00% (2690/5380)\n",
      "Fininshed: 60.00% (3228/5380)\n",
      "Fininshed: 70.00% (3766/5380)\n",
      "Fininshed: 80.00% (4304/5380)\n",
      "Fininshed: 90.00% (4842/5380)\n",
      "Fininshed: 100.00% (5380/5380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|████████████████████████▎                                                | 1/3 [45:36<1:31:12, 2736.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/5380)\n",
      "Fininshed: 10.00% (538/5380)\n",
      "Fininshed: 20.00% (1076/5380)\n",
      "Fininshed: 30.00% (1614/5380)\n",
      "Fininshed: 40.00% (2152/5380)\n",
      "Fininshed: 50.00% (2690/5380)\n",
      "Fininshed: 60.00% (3228/5380)\n",
      "Fininshed: 70.00% (3766/5380)\n",
      "Fininshed: 80.00% (4304/5380)\n",
      "Fininshed: 90.00% (4842/5380)\n",
      "Fininshed: 100.00% (5380/5380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|████████████████████████████████████████████████▋                        | 2/3 [1:33:10<46:11, 2771.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/5380)\n",
      "Fininshed: 10.00% (538/5380)\n",
      "Fininshed: 20.00% (1076/5380)\n",
      "Fininshed: 30.00% (1614/5380)\n",
      "Fininshed: 40.00% (2152/5380)\n",
      "Fininshed: 50.00% (2690/5380)\n",
      "Fininshed: 60.00% (3228/5380)\n",
      "Fininshed: 70.00% (3766/5380)\n",
      "Fininshed: 80.00% (4304/5380)\n",
      "Fininshed: 90.00% (4842/5380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 18:37:57 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "02/23/2020 18:37:57 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 100.00% (5380/5380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████| 3/3 [2:18:30<00:00, 2770.06s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# token tensor\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], \n",
    "                            dtype=torch.long)\n",
    "# Masks tensor\n",
    "all_input_mask = torch.tensor([f.segment_ids for f in train_features],\n",
    "                             dtype=torch.long)\n",
    "# segment tensor\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features],\n",
    "                              dtype=torch.long)\n",
    "# tensor the input\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features],\n",
    "                            dtype=torch.long)\n",
    "\n",
    "# add the encoder together-->\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids)\n",
    "\n",
    "# train the random sampling\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# generate the batch size using dataloader\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "model.train()\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    total_step = len(train_data) // train_batch_size\n",
    "    ten_percent_step = total_step // 10\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            # gpu parallel computing\n",
    "            loss = loss.mean()   # mean() to average on multi-gpu.\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "        if step % ten_percent_step == 0:\n",
    "            print(\"Fininshed: {:.2f}% ({}/{})\".format(step/total_step*100, step, total_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a trained model and the associated configuration\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, 'WEIGHTS_NAME')\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "output_config_file = os.path.join(output_dir, 'CONFIG_NAME')\n",
    "\n",
    "with open(output_config_file, 'w') as f:\n",
    "    f.write(model_to_save.config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a trained model and config that you have fine-tuned\n",
    "config = BertConfig(output_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(output_config_file)\n",
    "model = BertForSequenceClassification(config, num_labels=len(label_list))\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "model.to(device)\n",
    "\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation for the model --> Fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 18:38:56 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   guid: val\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   tokens: [CLS] 喝 咖 啡 致 癌 ？ ！ 最 功 德 律 师 挑 战 星 巴 克 ！ 上 诉 8 年 只 为 一 条 标 签 [SEP] 什 么 ！ 星 巴 克 最 大 丑 闻 曝 光 ， 他 家 咖 啡 竟 然 致 癌 ？ ！ [SEP]\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_ids: 101 1600 1476 1565 5636 4617 8043 8013 3297 1216 2548 2526 2360 2904 2773 3215 2349 1046 8013 677 6401 129 2399 1372 711 671 3340 3403 5041 102 784 720 8013 3215 2349 1046 3297 1920 682 7319 3284 1045 8024 800 2157 1476 1565 4994 4197 5636 4617 8043 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   guid: val\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   tokens: [CLS] 只 需 一 粒 阿 莫 西 林 晚 上 敷 敷 脸 ， 皮 肤 紧 俏 嫩 白 立 马 年 轻 20 岁 [SEP] 巧 用 阿 莫 西 林 抹 脸 3 次 ， 皮 肤 又 白 又 嫩 ， 脸 上 显 白 年 轻 20 岁 [SEP]\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_ids: 101 1372 7444 671 5108 7350 5811 6205 3360 3241 677 3148 3148 5567 8024 4649 5502 5165 918 2075 4635 4989 7716 2399 6768 8113 2259 102 2341 4500 7350 5811 6205 3360 2851 5567 124 3613 8024 4649 5502 1348 4635 1348 2075 8024 5567 677 3227 4635 2399 6768 8113 2259 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   guid: val\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   tokens: [CLS] 佟 丽 娅 微 信 记 录 曝 光 疑 离 婚 称 要 逼 自 己 一 次 [SEP] 佟 丽 娅 回 应 ： 重 新 开 始 ！ 网 友 ： 要 离 婚 ？ 陈 思 成 出 来 居 然 辟 谣 了 ！ [SEP]\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_ids: 101 871 714 2017 2544 928 6381 2497 3284 1045 4542 4895 2042 4917 6206 6873 5632 2346 671 3613 102 871 714 2017 1726 2418 8038 7028 3173 2458 1993 8013 5381 1351 8038 6206 4895 2042 8043 7357 2590 2768 1139 3341 2233 4197 6792 6469 749 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   guid: val\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   tokens: [CLS] 班 主 任 直 言 ： 孩 子 一 旦 出 现 这 6 种 现 象 ， 成 绩 绝 对 一 落 千 丈 ！ 跑 不 了 [SEP] 警 惕 ! 凡 有 这 6 个 现 象 的 孩 子 , 成 绩 都 一 落 千 丈 ! 家 长 们 要 注 意 ! [SEP]\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_ids: 101 4408 712 818 4684 6241 8038 2111 2094 671 3190 1139 4385 6821 127 4905 4385 6496 8024 2768 5327 5318 2190 671 5862 1283 675 8013 6651 679 749 102 6356 2664 106 1127 3300 6821 127 702 4385 6496 4638 2111 2094 117 2768 5327 6963 671 5862 1283 675 106 2157 7270 812 6206 3800 2692 106 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   label: agreed (id = 1)\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   guid: val\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   tokens: [CLS] 《 印 囧 》 即 将 上 映 ， 看 这 演 员 阵 容 ， 是 要 破 《 战 狼 2 》 票 房 纪 录 啊 ？ [SEP] 《 印 囧 》 即 将 上 映 ， 看 这 演 员 阵 容 ， 这 要 破 战 狼 2 票 房 的 节 奏 ！ [SEP]\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_ids: 101 517 1313 1733 518 1315 2199 677 3216 8024 4692 6821 4028 1447 7347 2159 8024 3221 6206 4788 517 2773 4331 123 518 4873 2791 5279 2497 1557 8043 102 517 1313 1733 518 1315 2199 677 3216 8024 4692 6821 4028 1447 7347 2159 8024 6821 6206 4788 2773 4331 123 4873 2791 4638 5688 1941 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:38:56 - INFO - run_classifier -   label: agreed (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# use the evaluation group data for the calculation\n",
    "eval_examples = val_examples\n",
    "eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "# logger.info(\"**** Running evaluation ****\")\n",
    "# logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "# logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "## Index tensor\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "# mask tensor\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "# segemnt tensor\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "# all label tensor\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "### Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "# Run the model\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0 , 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 18:45:40 - INFO - run_classifier -   **** Eval result ****\n",
      "02/23/2020 18:45:40 - INFO - run_classifier -     eval_accuracy = 0.7060471729675477\n",
      "02/23/2020 18:45:40 - INFO - run_classifier -     eval_loss = 0.5751065005832178\n",
      "02/23/2020 18:45:40 - INFO - run_classifier -     global_step = 16143\n",
      "02/23/2020 18:45:40 - INFO - run_classifier -     loss = 0.5372565723367172\n"
     ]
    }
   ],
   "source": [
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    # Encoder - layer 1\n",
    "    input_ids = input_ids.to(device)\n",
    "    # Encoder - layer 2\n",
    "    input_mask = input_mask.to(device)\n",
    "    # Encoder - layer 3\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    # Decoder - layer\n",
    "    label_ids = label_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss / nb_tr_steps\n",
    "result = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"eval_accuracy\": eval_accuracy,\n",
    "    \"global_step\": global_step,\n",
    "    \"loss\": loss\n",
    "}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, 'w') as writer:\n",
    "    logger.info(\"**** Eval result ****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, examples, label_list, eval_batch_size=128):\n",
    "    model.to(device)\n",
    "    eval_examples = examples\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    # index tensor\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    # mask tensor\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    # segment tensor\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    # label tensor\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    # group them together\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    res = []\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # add the training, evaluation and \n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        res.extend(logits.argmax(-1))\n",
    "        nb_eval_steps += 1\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2020 18:46:58 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   guid: test\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   tokens: [CLS] 萨 拉 赫 人 气 爆 棚 ! 埃 及 总 统 大 选 未 参 选 获 百 万 选 票 现 任 总 统 压 力 山 大 [SEP] 辟 谣 ！ 里 昂 官 方 否 认 费 基 尔 加 盟 利 物 浦 ， 难 道 是 价 格 没 谈 拢 ？ [SEP]\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_ids: 101 5855 2861 6622 782 3698 4255 3476 106 1812 1350 2600 5320 1920 6848 3313 1346 6848 5815 4636 674 6848 4873 4385 818 2600 5320 1327 1213 2255 1920 102 6792 6469 8013 7027 3203 2135 3175 1415 6371 6589 1825 2209 1217 4673 1164 4289 3855 8024 7410 6887 3221 817 3419 3766 6448 2879 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   guid: test\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   tokens: [CLS] 萨 达 姆 被 捕 后 告 诫 美 国 的 一 句 话 ， 发 人 深 思 [SEP] 10 大 最 让 美 国 人 相 信 的 荒 诞 谣 言 ， 如 蜥 蜴 人 掌 控 着 美 国 [SEP]\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_ids: 101 5855 6809 1990 6158 2936 1400 1440 6425 5401 1744 4638 671 1368 6413 8024 1355 782 3918 2590 102 8108 1920 3297 6375 5401 1744 782 4685 928 4638 5774 6414 6469 6241 8024 1963 6060 6062 782 2958 2971 4708 5401 1744 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   guid: test\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   tokens: [CLS] 萨 达 姆 此 项 计 划 没 有 此 国 破 坏 的 话 ， 美 国 还 会 对 伊 拉 克 发 动 战 争 吗 [SEP] 萨 达 姆 被 捕 后 告 诫 美 国 的 一 句 话 ， 发 人 深 思 [SEP]\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_ids: 101 5855 6809 1990 3634 7555 6369 1153 3766 3300 3634 1744 4788 1776 4638 6413 8024 5401 1744 6820 833 2190 823 2861 1046 1355 1220 2773 751 1408 102 5855 6809 1990 6158 2936 1400 1440 6425 5401 1744 4638 671 1368 6413 8024 1355 782 3918 2590 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   guid: test\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   tokens: [CLS] 萨 达 姆 被 捕 后 告 诫 美 国 的 一 句 话 ， 发 人 深 思 [SEP] 被 绞 刑 处 死 的 萨 达 姆 是 替 身 ？ 他 的 此 男 人 举 动 击 破 替 身 谣 言 ！ [SEP]\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_ids: 101 5855 6809 1990 6158 2936 1400 1440 6425 5401 1744 4638 671 1368 6413 8024 1355 782 3918 2590 102 6158 5319 1152 1905 3647 4638 5855 6809 1990 3221 3296 6716 8043 800 4638 3634 4511 782 715 1220 1140 4788 3296 6716 6469 6241 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   label: unrelated (id = 0)\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   *** Example ***\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   guid: test\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   tokens: [CLS] 萨 达 姆 被 捕 后 告 诫 美 国 的 一 句 话 ， 发 人 深 思 [SEP] 中 国 川 贝 枇 杷 膏 在 美 国 受 到 热 捧 ？ 纯 属 谣 言 ！ [SEP]\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_ids: 101 5855 6809 1990 6158 2936 1400 1440 6425 5401 1744 4638 671 1368 6413 8024 1355 782 3918 2590 102 704 1744 2335 6564 3355 3349 5601 1762 5401 1744 1358 1168 4178 2942 8043 5283 2247 6469 6241 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/23/2020 18:46:58 - INFO - run_classifier -   label: unrelated (id = 0)\n"
     ]
    }
   ],
   "source": [
    "res = predict(model, tokenizer, test_examples, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unrelated', 'agreed', 'disagreed']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80126"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>321187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>321190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>321189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>321193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>321191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Category\n",
       "0  321187         0\n",
       "1  321190         0\n",
       "2  321189         0\n",
       "3  321193         0\n",
       "4  321191         0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Category'] = res\n",
    "\n",
    "\n",
    "submission = test \\\n",
    "    .loc[:, ['Category']] \\\n",
    "    .reset_index()\n",
    "\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80126"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.Category.values * test.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the fake new prediction is: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_acc = 0\n",
    "total = 0\n",
    "\n",
    "test_val = test.label.values\n",
    "\n",
    "\n",
    "for i in range(len(test_val)):\n",
    "    if test_val[i] == res[i]:\n",
    "        num_acc += 1\n",
    "    total += 1\n",
    "\n",
    "print('accuracy of the fake new prediction is:', num_acc / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80126"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.label = submission.Category.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>321187</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321190</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321189</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321193</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321191</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321285</td>\n",
       "      <td>著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴</td>\n",
       "      <td>鲁豫曝出惊天身价 骨瘦如柴13岁初恋遭家暴 今又嫁给初恋</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321282</td>\n",
       "      <td>著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴</td>\n",
       "      <td>嫁给外国人的七位女星，只有李玟夫妇恩爱如初，陈鲁豫被“家暴”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321284</td>\n",
       "      <td>著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴</td>\n",
       "      <td>皮包骨的陈鲁豫，嫁给外国老公遭家暴，但现在依旧坚强！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321288</td>\n",
       "      <td>著名女高音歌唱家马玉涛被曝离世，《马儿啊！你慢些走》曾唱响全国</td>\n",
       "      <td>84岁歌唱家马玉涛“被死亡”，女儿发声辟谣：妈妈身体近况非常健康</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321283</td>\n",
       "      <td>著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴</td>\n",
       "      <td>嫁给外国人的六位女星，李玟夫妇恩爱如初，鲁豫遭家暴后回国发展</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title1_zh                         title2_zh  \\\n",
       "id                                                                          \n",
       "321187  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大       辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190              萨达姆被捕后告诫美国的一句话，发人深思         10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗               萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321193              萨达姆被捕后告诫美国的一句话，发人深思       被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！   \n",
       "321191              萨达姆被捕后告诫美国的一句话，发人深思              中国川贝枇杷膏在美国受到热捧？纯属谣言！   \n",
       "...                                 ...                               ...   \n",
       "321285     著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴      鲁豫曝出惊天身价 骨瘦如柴13岁初恋遭家暴 今又嫁给初恋   \n",
       "321282     著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴    嫁给外国人的七位女星，只有李玟夫妇恩爱如初，陈鲁豫被“家暴”   \n",
       "321284     著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴        皮包骨的陈鲁豫，嫁给外国老公遭家暴，但现在依旧坚强！   \n",
       "321288  著名女高音歌唱家马玉涛被曝离世，《马儿啊！你慢些走》曾唱响全国  84岁歌唱家马玉涛“被死亡”，女儿发声辟谣：妈妈身体近况非常健康   \n",
       "321283     著名女主持人鲁豫体重仅80斤，早年时还曾惨遭美国老公家暴    嫁给外国人的六位女星，李玟夫妇恩爱如初，鲁豫遭家暴后回国发展   \n",
       "\n",
       "        label  \n",
       "id             \n",
       "321187      0  \n",
       "321190      0  \n",
       "321189      0  \n",
       "321193      0  \n",
       "321191      0  \n",
       "...       ...  \n",
       "321285      1  \n",
       "321282      1  \n",
       "321284      1  \n",
       "321288      0  \n",
       "321283      1  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>401559</td>\n",
       "      <td>｜提醒｜没考驾照的人真要哭了……4月1日起，又有新规将实施！</td>\n",
       "      <td>驾考今起将计时收费？谣言！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401560</td>\n",
       "      <td>｜稻草推荐｜央视邀请刀郎参加2018春晚，请他唱这首歌</td>\n",
       "      <td>央视邀请小沈阳参加2018春晚翻唱刀郎这首歌，云朵激动哭到晕厥！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401562</td>\n",
       "      <td>｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》</td>\n",
       "      <td>于文华陪朱之文回家种地，两人甜蜜合唱，朱之文老婆哭到断肠！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401563</td>\n",
       "      <td>｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》</td>\n",
       "      <td>朱之文于文华婚后，回到农村老家种菜，唱歌秀恩爱，前妻翻脸痛哭</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401564</td>\n",
       "      <td>｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》</td>\n",
       "      <td>朱之文婚后带于文华回家，遭村民围观，两人合唱一首听哭全村人！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title1_zh                         title2_zh  \\\n",
       "id                                                                         \n",
       "401559  ｜提醒｜没考驾照的人真要哭了……4月1日起，又有新规将实施！                     驾考今起将计时收费？谣言！   \n",
       "401560     ｜稻草推荐｜央视邀请刀郎参加2018春晚，请他唱这首歌  央视邀请小沈阳参加2018春晚翻唱刀郎这首歌，云朵激动哭到晕厥！   \n",
       "401562    ｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》     于文华陪朱之文回家种地，两人甜蜜合唱，朱之文老婆哭到断肠！   \n",
       "401563    ｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》    朱之文于文华婚后，回到农村老家种菜，唱歌秀恩爱，前妻翻脸痛哭   \n",
       "401564    ｜逆境必推精品｜朱之文、于文华婚后第一次牵手合唱《驼铃》    朱之文婚后带于文华回家，遭村民围观，两人合唱一首听哭全村人！   \n",
       "\n",
       "        label  \n",
       "id             \n",
       "401559      0  \n",
       "401560      1  \n",
       "401562      1  \n",
       "401563      1  \n",
       "401564      1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
